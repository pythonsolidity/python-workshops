{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WOw8yMd1VlnD"
   },
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NvUGC8QQV6bV"
   },
   "source": [
    "### Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wfFEXZC0WS-V"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fhYaZ-ENV_c5"
   },
   "source": [
    "### Reading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aqHTg9bxWT_u"
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('marketing.csv')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate Independent variables (X) and dependent variable (y)\n",
    "\n",
    "X = dataset.iloc[:, 0:3].values\n",
    "y = dataset.iloc[:, 3].values\n",
    "\n",
    "print(X,'\\n\\n', y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace missing values by the column/variable average (mean)\n",
    "# Import Class --> Create Object --> Fit Object to Data --> Transform Data\n",
    "\n",
    "from sklearn.impute import SimpleImputer                            #import the SimpleImputer class\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')     #create the imputer object\n",
    "\n",
    "imputer.fit(X[:, 1:])                          #fit the object to the data\n",
    "X[:, 1:] = imputer.transform(X[:, 1:])        #transform data\n",
    "\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding independent variables\n",
    "# Import Class --> Create Object --> Fit Object to Data --> Transform Data\n",
    "\n",
    "from sklearn.compose import ColumnTransformer       # import class\n",
    "from sklearn.preprocessing import OneHotEncoder     # import class\n",
    "\n",
    "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(drop = 'first'), [0])], remainder='passthrough')    \n",
    "#create object\n",
    "\n",
    "X = np.array(ct.fit_transform(X))                   # fit object to data and transform data\n",
    "\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding dependent variable because this is a classification problem. The dependent variable is categorical.\n",
    "# Import Class --> Create Object --> Fit Object to Data --> Transform Data\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder   #import class\n",
    "le = LabelEncoder()                              #create object\n",
    "y = le.fit_transform(y)                          #fit and transform\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjusting the scales of independent variables \n",
    "# Import Class --> Create Object --> Fit Object to Data --> Transform Data\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler   # import class\n",
    "sc = StandardScaler()                              # create object\n",
    "X = sc.fit_transform(X)                            # fit and transform \n",
    "\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(X)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3abSxRqvWEIB"
   },
   "source": [
    "### Splitting the dataset into the Training set and Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hm48sif-WWsh"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classification method we use here is called <b>Logistic Regression</b>. Note that even though its name has the word 'regression', it is a classification technique.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use logistic regression. \n",
    "# Can also use support vector machine, decision trees or random forest classification.  \n",
    "\n",
    "# Import Class --> Create Object --> Fit Object to Data --> Predict\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression   # import class\n",
    "classifier = LogisticRegression()                     # create object\n",
    "classifier.fit(X_train, y_train)                      # fit object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions for test data\n",
    "\n",
    "y_pred = classifier.predict(X_test)    # predict\n",
    "\n",
    "err = y_pred - y_test\n",
    "\n",
    "df = pd.DataFrame({'y_pred': y_pred, 'y_test': y_test, 'error': err})     # compare\n",
    "\n",
    "# The following command allows you to set the number of rows of the data frame that will be displayed\n",
    "# If you ever wish to change the number of rows back to 10, you would need to execute the command again\n",
    "# with 100 replaced by 10\n",
    "# pd.set_option('display.max_rows', 100)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting the errors. The errors correspond to the number of 1's and -1's in the err column.\n",
    "print('False positives: ', list(err).count(1))\n",
    "print('False negatives: ', list(err).count(-1))\n",
    "\n",
    "# accuracy = (total - number of errors) x 100 / total\n",
    "print('Accuracy = ', (80 - list(err).count(1) - list(err).count(1))*100/80, '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of computing the number of errors manually, you can use the <b>confusion matrix</b>. It displays correct predictions in the top-left and bottom-right cells, and errors in top-right and bottom-left cells. For example, the following confusion matrix shows \n",
    "<img src=\"confusion.png\" width=360 />\n",
    "150 correct predictions and 15 errors. So \n",
    "\\begin{equation*}\n",
    "Accuracy = \\dfrac{correct\\ predictions}{total\\ predictions}\\times 100 = \\dfrac{150}{165} \\times 100 = 91\\%\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.heatmap(cm, annot=True, center = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forecasting (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict if a 28 year old male with 50,000 salary would make the purchase\n",
    "# What happens if you change the salary to 200,000?\n",
    "\n",
    "Z = sc.transform([[1, 28, 50000]])       # scale data by .transform() method but NOT .fit_transform()\n",
    "\n",
    "prediction = classifier.predict(Z)\n",
    "print(Z, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOD2/gZgY69JdiiGJVNfu7s",
   "collapsed_sections": [],
   "name": "data_preprocessing_template.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
